---
layout: post
title: MLE, my notes on estimation
published: true
comments: true
---

In my stat's class, we are using Larsen's 6th edition of `An Introduction to Mathematical Statistics and Its Applications.` This week, we are doing chapter 5, estimators. If you want a good rundown of MLE, there are tons of other resources:

> The Method of Maximum Likelihood MLE
> 
> Method of Moments
> 
> Choosing a good estimator: minimum variance/sufficent and consistant estimators

# Likelihood Function v. Probability Function
Last semester, we did a lot of exploration with the [iris dataset](https://www.kaggle.com/datasets/uciml/iris). Briefly, the iris dataset contains data on 150 iris flowers. There are 3 different species of flowers (50 of each iris species) and each flower has 4 measurements/features (petal width/length, sepal width/length). We played with different ways to cluster the iris by species by looking at the more distinguishable features. *Spoiler, petal length and petal width varies the most between iris species!*

One clustering method we used was Gaussian Mixture Models (GMM). The whole dataset was the sum of 3 different iris species so our GMM was the sum of 3 normal distributions. To write the normal distribution for each of the iris species, all you need is the mean and standard deviation for each species. Lastly, you use the GMM to calculate the likelihood of the each data of being each of the 3 species and sort the data into the species with the highest likelihood.

One could simply calculate parameter values for the species' distributions but we took it one step more and wanted to find the *best* estimators for mean and standard deviation! We used expectation maximization, a method that calculates the conditional probability of the cluster using guesstimated parameters, given the data. The algorithm cycles, modifying the parameters until the probability converges.

$$P(species|data)=\frac{P(species) * \left(pdf \text{ of species with guesstimate parameters}\right)}{P(data)}$$

While I was writing up the homework, I found myself interchanging "probability" and "likelihood." I had taken stats before, but even so, I could not quite remember the distinction between likelihood and probability!

To put shortly:

`Likelihood function: how likely is the hypothesis for varying parameter values given this data?`

`Probability function: probability for the data, given the hypothesis`

```
HELLLOOO
```
# $$\skew{-1}\hat\theta}$$
Additionally, we want to find the parameter value that maximizes the likelihood. In this example, we numerically calculated the likelihood for different parameter values. Each iteration, we refined the parameters until the change in previous and current likelihood was zero.

To maximize $\theta$ empirically, we find where the change in the likelihood function $\ell(\theta)$ zero, i.e., where the derivative is zero!

Well, \mathcal{L}(\theta) is the product of the $pdf$s for all outcomes in the range. 

$$\mathcal{L} (\theta) = \prod_{i}^{\infty} pdf $$


