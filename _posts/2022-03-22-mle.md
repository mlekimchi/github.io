---
layout: post
title: MLE: my notes on estimation
published: true
comments: true
---

In my stat's class, we are using Larsen's 6th edition of `An Introduction to Mathematical Statistics and Its Applications.` This week, we are doing chapter 5, estimators.

> The Method of Maximum Likelihood MLE
> 
> Method of Moments
> 
> Choosing a good estimator: minimum variance/sufficent and consistant estimators

# Likelihood Function v. Probability Function
Last semester, we did a lot of exploration with the [iris dataset](https://www.kaggle.com/datasets/uciml/iris). Briefly, the iris dataset contains data on 150 iris flowers. There are 3 different species of flowers (50 of each iris species) and each flower has 4 measurements/features (petal width/length, sepal width/length). We played with different ways to cluster the iris by species by looking at the more distinguishable features. *Spoiler, petal length and petal width varies the most between iris species!*

One clustering method we used was Gaussian Mixture Models (GMM). The whole dataset was the sum of 3 different iris species so our GMM was the sum of 3 normal distributions. To write the normal distribution for each of the iris species, all you need is the mean and standard deviation for each species. Lastly, you use the GMM to calculate the likelihood of the each data of being each of the 3 species and sort the data into the species with the highest likelihood.

One could simply calculate parameter values for the species' distributions but we took it one step more and wanted to find the *best* estimators for mean and standard deviation! We used expectation maximization, a method that calculates the conditional probability of the cluster using guesstimated parameters, given the data. The algorithm cycles, modifying the parameters until the probability converges.

$$P(species|data)=\frac{P(cluster) * \left(pdf \text{ of species with guesstimate parameters}\right)}{P(data)}$$

While I was writing up the homework, I found myself interchanging "probability" and "likelihood." I had taken stats before, but even so, I could not quite remember the distinction between likelihood and probability!

To put shortly:

`Likelihood function: how likely is the hypothesis for varying parameter values given this data?

`Probability function: probability for the data, given the hypothesis

```
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
```

Note that this script should appear before any LaTex inputs in your file.

It is tedious to add this script for every md file, so we can build a `_html` file in the `_includes` fold, named `mathjax.html`, 
which includes the very script mentioned above. Then we only need add one short sentence in the md file to get everything set: `{`%` include mathjax.html `%`}`. No configuration in `_config.yml` is needed! 

Yet it is still time-consuming to write the `include` things in every post. Here comes the final solution: change your `base.html` file in the `_layouts` folder in the template. Include the   `{`%` include mathjax.html `%`}` sentence into the `base.html` file, then everything is set and there is no need write the script again in every md file.


You can check my [GitHub repo](https://github.com/xkdog/xkdog.github.io) to see how this works.

See how beautiful these LaTex formulas are:

$$\sum_{i=0}^n i^2 = \frac{n(n+1)(2n+1)}{6}$$


$$ F(x) =\int _{-\infty} ^{x} \frac {1} {\sqrt {2 \pi} \sigma} \mathrm {e}^ \frac {-(t-\mu)^2} {2 \sigma ^2 } \mathrm {d} t $$



